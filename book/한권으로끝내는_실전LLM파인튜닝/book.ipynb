{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.2 데이터 준비와 모델 구성",
   "id": "23e96792026ec180"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!git clone https://github.com/wikibook/llm-finetuning.git",
   "id": "3f99ab1488b5f814"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install -q datasets",
   "id": "91ca67de06820a43"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"daekeun-ml/naver-news-summarization-ko\")"
   ],
   "id": "83f1e239edd98d09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = dataset\n",
    "data"
   ],
   "id": "c7521f71653bc85d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data['train']['document'][0]",
   "id": "23be7750a9f99edd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "ko_text = \"\".join(data[\"train\"][\"document\"])\n",
    "ko_chars = sorted(list(set(ko_text)))\n",
    "ko_vocab_size = len(ko_chars)\n",
    "\n",
    "print(\"총 글자 수 :\", ko_vocab_size)"
   ],
   "id": "c219f9e5ec3ec7a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "character_to_ids = {char: i for i, char in enumerate(ko_chars)}\n",
    "ids_to_character = {i: char for i, char in enumerate(ko_chars)}\n",
    "\n",
    "token_encode = lambda s:[character_to_ids[c] for c in s]\n",
    "token_decode = lambda l: \"\".join([ids_to_character[i] for i in l])\n",
    "\n",
    "print(token_encode(\"안녕하세요. 함께 인공지능을 배워봅시다.\"))\n",
    "print(token_decode(token_encode(\"안녕하세요. 함께 인공지능을 배워봅시다.\")))"
   ],
   "id": "3ee7b007dc785c7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install torch",
   "id": "15e011206a39a9cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "\n",
    "tokenized_data = torch.tensor(token_encode(ko_text), dtype=torch.long)\n",
    "print(tokenized_data.shape, tokenized_data.dtype)\n",
    "print(tokenized_data[:100])"
   ],
   "id": "6bbba878a8fe5cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "n = int(0.9 * len(tokenized_data))\n",
    "train_dataset = tokenized_data[:n]\n",
    "test_dataset = tokenized_data[n:]"
   ],
   "id": "d1a61a1102368e43"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "torch.manual_seed(1234)\n",
    "\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def batch_function(mode):\n",
    "    dataset  = train_dataset if mode == \"train\" else test_dataset\n",
    "    idx = torch.randint(len(dataset) - block_size, (batch_size,))\n",
    "\n",
    "    x = torch.stack([dataset[index : index + block_size] for index in idx])\n",
    "    y = torch.stack([dataset[index + 1 : index + block_size + 1] for index in idx])\n",
    "\n",
    "    return x, y\n",
    "\n",
    "example_x, example_y = batch_function(\"train\")"
   ],
   "id": "7e05e8edd31e0281"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.3 언어 모델 만들기",
   "id": "456a2b6375958417"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class semiGPT(nn.Module):\n",
    "    def __init__(self, vocab_length):\n",
    "        super().__init__()\n",
    "        self.embedding_token_table = nn.Embedding(vocab_length, vocab_length)\n",
    "\n",
    "    def forward(self, inputs, targets=None):\n",
    "        logits = self.embedding_token_table(inputs)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch, seq_length, vocab_length = logits.shape\n",
    "            logits = logits.view(batch * seq_length, vocab_length)\n",
    "            targets = targets.view(batch*seq_length)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self.forward(inputs)\n",
    "            logits = logits[:, -1, :]  # 마지막 토큰의 logits만 사용\n",
    "            print(logits.shape)\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_inputs = torch.multinomial(probs, num_samples=1)\n",
    "            inputs = torch.cat((inputs, next_inputs), dim=1)\n",
    "        return inputs\n",
    "\n",
    "model = semiGPT(ko_vocab_size)\n",
    "logits, loss = model(example_x, example_y)\n",
    "print(loss)\n",
    "\n",
    "token_decode(model.generate(torch.zeros((1,1),\n",
    "                                        dtype=torch.long),\n",
    "                            max_new_tokens=10)[0].tolist())"
   ],
   "id": "441d58cead0cf601"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.4 Optimizer 추가하기",
   "id": "a9d734479b426353"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "learning_rate = 1e-2\n",
    "model = semiGPT(ko_vocab_size)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ],
   "id": "e3fc25cafd17eef1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "for steps in tqdm(range(10000)):\n",
    "    example_x, example_y = batch_function(\"train\")\n",
    "    logits, loss = model(example_x, example_y)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ],
   "id": "fc6d55d69b2b670e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ],
   "id": "8d5e7e9a3760d7c9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T02:38:56.003223Z",
     "start_time": "2025-07-03T02:38:55.989948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def batch_function(mode):\n",
    "    dataset  = train_dataset if mode == \"train\" else test_dataset\n",
    "    idx = torch.randint(len(dataset) - block_size, (batch_size,))\n",
    "\n",
    "    x = torch.stack([dataset[index : index + block_size] for index in idx])\n",
    "    y = torch.stack([dataset[index + 1 : index + block_size + 1] for index in idx])\n",
    "\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ],
   "id": "f30d5b93bb7b53d7",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T02:40:32.008194Z",
     "start_time": "2025-07-03T02:40:32.001943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_iteration = 50000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "eval_iteration = 200"
   ],
   "id": "3687f839acd1ae5b",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T02:47:03.662527Z",
     "start_time": "2025-07-03T02:47:03.653292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def compute_loss_metrics():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for mode in [\"train\", \"eval\"]:\n",
    "        losses = torch.zeros(eval_iteration)\n",
    "        for k in range(eval_iteration):\n",
    "            inputs, targets = batch_function(mode)\n",
    "            logits, loss = model(inputs, targets)\n",
    "            losses[k] = loss.item()\n",
    "        out[mode] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ],
   "id": "fd01ee1232f6a79c",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T02:58:51.561122Z",
     "start_time": "2025-07-03T02:47:15.951351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for step in range(max_iteration):\n",
    "    if step % eval_interval == 0:\n",
    "        losses = compute_loss_metrics()\n",
    "        print(f\"Step {step}, Train Loss: {losses['train']:.4f}, val Loss: {losses['eval']:.4f}\")\n",
    "\n",
    "    example_x, example_y = batch_function(\"train\")\n",
    "    logits, loss = model(example_x, example_y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "inputs = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(token_decode(model.generate(inputs, max_new_tokens=100)[0].tolist()))"
   ],
   "id": "c533ab00c64d7520",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Train Loss: 3.3945, val Loss: 3.4197\n",
      "Step 300, Train Loss: 3.4070, val Loss: 3.4426\n",
      "Step 600, Train Loss: 3.4236, val Loss: 3.4080\n",
      "Step 900, Train Loss: 3.4031, val Loss: 3.4192\n",
      "Step 1200, Train Loss: 3.4088, val Loss: 3.4182\n",
      "Step 1500, Train Loss: 3.4133, val Loss: 3.4178\n",
      "Step 1800, Train Loss: 3.4078, val Loss: 3.4151\n",
      "Step 2100, Train Loss: 3.4154, val Loss: 3.4132\n",
      "Step 2400, Train Loss: 3.4204, val Loss: 3.3878\n",
      "Step 2700, Train Loss: 3.3978, val Loss: 3.4118\n",
      "Step 3000, Train Loss: 3.4107, val Loss: 3.4079\n",
      "Step 3300, Train Loss: 3.3878, val Loss: 3.4001\n",
      "Step 3600, Train Loss: 3.3968, val Loss: 3.4118\n",
      "Step 3900, Train Loss: 3.3951, val Loss: 3.4164\n",
      "Step 4200, Train Loss: 3.4017, val Loss: 3.4064\n",
      "Step 4500, Train Loss: 3.4213, val Loss: 3.4084\n",
      "Step 4800, Train Loss: 3.4005, val Loss: 3.4023\n",
      "Step 5100, Train Loss: 3.4081, val Loss: 3.3958\n",
      "Step 5400, Train Loss: 3.3977, val Loss: 3.4008\n",
      "Step 5700, Train Loss: 3.4046, val Loss: 3.4053\n",
      "Step 6000, Train Loss: 3.4055, val Loss: 3.4093\n",
      "Step 6300, Train Loss: 3.3867, val Loss: 3.4180\n",
      "Step 6600, Train Loss: 3.3872, val Loss: 3.4069\n",
      "Step 6900, Train Loss: 3.4012, val Loss: 3.3983\n",
      "Step 7200, Train Loss: 3.3908, val Loss: 3.3984\n",
      "Step 7500, Train Loss: 3.4011, val Loss: 3.3926\n",
      "Step 7800, Train Loss: 3.4095, val Loss: 3.4012\n",
      "Step 8100, Train Loss: 3.4068, val Loss: 3.4000\n",
      "Step 8400, Train Loss: 3.3883, val Loss: 3.4006\n",
      "Step 8700, Train Loss: 3.3928, val Loss: 3.3939\n",
      "Step 9000, Train Loss: 3.3769, val Loss: 3.3997\n",
      "Step 9300, Train Loss: 3.4045, val Loss: 3.3856\n",
      "Step 9600, Train Loss: 3.3952, val Loss: 3.3793\n",
      "Step 9900, Train Loss: 3.3979, val Loss: 3.4072\n",
      "Step 10200, Train Loss: 3.3953, val Loss: 3.3840\n",
      "Step 10500, Train Loss: 3.3945, val Loss: 3.4157\n",
      "Step 10800, Train Loss: 3.3985, val Loss: 3.3955\n",
      "Step 11100, Train Loss: 3.3988, val Loss: 3.4050\n",
      "Step 11400, Train Loss: 3.4048, val Loss: 3.4045\n",
      "Step 11700, Train Loss: 3.3901, val Loss: 3.4021\n",
      "Step 12000, Train Loss: 3.3853, val Loss: 3.4027\n",
      "Step 12300, Train Loss: 3.4144, val Loss: 3.3939\n",
      "Step 12600, Train Loss: 3.4058, val Loss: 3.4016\n",
      "Step 12900, Train Loss: 3.3886, val Loss: 3.3998\n",
      "Step 13200, Train Loss: 3.3952, val Loss: 3.3989\n",
      "Step 13500, Train Loss: 3.3930, val Loss: 3.3960\n",
      "Step 13800, Train Loss: 3.3851, val Loss: 3.3862\n",
      "Step 14100, Train Loss: 3.3855, val Loss: 3.3940\n",
      "Step 14400, Train Loss: 3.3710, val Loss: 3.3951\n",
      "Step 14700, Train Loss: 3.3912, val Loss: 3.4076\n",
      "Step 15000, Train Loss: 3.4020, val Loss: 3.4034\n",
      "Step 15300, Train Loss: 3.4105, val Loss: 3.3983\n",
      "Step 15600, Train Loss: 3.3885, val Loss: 3.3915\n",
      "Step 15900, Train Loss: 3.3775, val Loss: 3.4006\n",
      "Step 16200, Train Loss: 3.4012, val Loss: 3.3926\n",
      "Step 16500, Train Loss: 3.3804, val Loss: 3.3859\n",
      "Step 16800, Train Loss: 3.3949, val Loss: 3.3988\n",
      "Step 17100, Train Loss: 3.3943, val Loss: 3.4020\n",
      "Step 17400, Train Loss: 3.3975, val Loss: 3.3943\n",
      "Step 17700, Train Loss: 3.3772, val Loss: 3.4088\n",
      "Step 18000, Train Loss: 3.3779, val Loss: 3.4038\n",
      "Step 18300, Train Loss: 3.3913, val Loss: 3.4107\n",
      "Step 18600, Train Loss: 3.3942, val Loss: 3.3994\n",
      "Step 18900, Train Loss: 3.3812, val Loss: 3.4135\n",
      "Step 19200, Train Loss: 3.3998, val Loss: 3.3765\n",
      "Step 19500, Train Loss: 3.3974, val Loss: 3.4106\n",
      "Step 19800, Train Loss: 3.3915, val Loss: 3.3953\n",
      "Step 20100, Train Loss: 3.3953, val Loss: 3.4130\n",
      "Step 20400, Train Loss: 3.3956, val Loss: 3.3859\n",
      "Step 20700, Train Loss: 3.3847, val Loss: 3.4094\n",
      "Step 21000, Train Loss: 3.4056, val Loss: 3.4124\n",
      "Step 21300, Train Loss: 3.4074, val Loss: 3.3845\n",
      "Step 21600, Train Loss: 3.3990, val Loss: 3.4089\n",
      "Step 21900, Train Loss: 3.4004, val Loss: 3.3953\n",
      "Step 22200, Train Loss: 3.3853, val Loss: 3.4016\n",
      "Step 22500, Train Loss: 3.3768, val Loss: 3.4004\n",
      "Step 22800, Train Loss: 3.4100, val Loss: 3.4024\n",
      "Step 23100, Train Loss: 3.3815, val Loss: 3.4016\n",
      "Step 23400, Train Loss: 3.3963, val Loss: 3.4038\n",
      "Step 23700, Train Loss: 3.4027, val Loss: 3.3888\n",
      "Step 24000, Train Loss: 3.3803, val Loss: 3.3872\n",
      "Step 24300, Train Loss: 3.3889, val Loss: 3.4027\n",
      "Step 24600, Train Loss: 3.3715, val Loss: 3.4149\n",
      "Step 24900, Train Loss: 3.3766, val Loss: 3.3910\n",
      "Step 25200, Train Loss: 3.3927, val Loss: 3.3950\n",
      "Step 25500, Train Loss: 3.3873, val Loss: 3.4055\n",
      "Step 25800, Train Loss: 3.4095, val Loss: 3.4007\n",
      "Step 26100, Train Loss: 3.3940, val Loss: 3.4270\n",
      "Step 26400, Train Loss: 3.3923, val Loss: 3.4018\n",
      "Step 26700, Train Loss: 3.3943, val Loss: 3.3847\n",
      "Step 27000, Train Loss: 3.3986, val Loss: 3.4124\n",
      "Step 27300, Train Loss: 3.3852, val Loss: 3.4081\n",
      "Step 27600, Train Loss: 3.3859, val Loss: 3.3909\n",
      "Step 27900, Train Loss: 3.3831, val Loss: 3.3951\n",
      "Step 28200, Train Loss: 3.3939, val Loss: 3.4058\n",
      "Step 28500, Train Loss: 3.3945, val Loss: 3.4049\n",
      "Step 28800, Train Loss: 3.3971, val Loss: 3.3929\n",
      "Step 29100, Train Loss: 3.3905, val Loss: 3.4001\n",
      "Step 29400, Train Loss: 3.3918, val Loss: 3.4075\n",
      "Step 29700, Train Loss: 3.3922, val Loss: 3.3839\n",
      "Step 30000, Train Loss: 3.4049, val Loss: 3.3954\n",
      "Step 30300, Train Loss: 3.3954, val Loss: 3.3990\n",
      "Step 30600, Train Loss: 3.3783, val Loss: 3.3981\n",
      "Step 30900, Train Loss: 3.3821, val Loss: 3.4174\n",
      "Step 31200, Train Loss: 3.3759, val Loss: 3.4076\n",
      "Step 31500, Train Loss: 3.4014, val Loss: 3.4092\n",
      "Step 31800, Train Loss: 3.3890, val Loss: 3.4128\n",
      "Step 32100, Train Loss: 3.3853, val Loss: 3.4156\n",
      "Step 32400, Train Loss: 3.3848, val Loss: 3.4093\n",
      "Step 32700, Train Loss: 3.3840, val Loss: 3.3934\n",
      "Step 33000, Train Loss: 3.3883, val Loss: 3.4127\n",
      "Step 33300, Train Loss: 3.3806, val Loss: 3.4042\n",
      "Step 33600, Train Loss: 3.3734, val Loss: 3.4003\n",
      "Step 33900, Train Loss: 3.3928, val Loss: 3.3811\n",
      "Step 34200, Train Loss: 3.3853, val Loss: 3.4073\n",
      "Step 34500, Train Loss: 3.3989, val Loss: 3.4132\n",
      "Step 34800, Train Loss: 3.3813, val Loss: 3.3983\n",
      "Step 35100, Train Loss: 3.3892, val Loss: 3.3944\n",
      "Step 35400, Train Loss: 3.3966, val Loss: 3.4015\n",
      "Step 35700, Train Loss: 3.3851, val Loss: 3.3984\n",
      "Step 36000, Train Loss: 3.3912, val Loss: 3.4162\n",
      "Step 36300, Train Loss: 3.3993, val Loss: 3.3943\n",
      "Step 36600, Train Loss: 3.3948, val Loss: 3.3946\n",
      "Step 36900, Train Loss: 3.3920, val Loss: 3.4086\n",
      "Step 37200, Train Loss: 3.3917, val Loss: 3.4037\n",
      "Step 37500, Train Loss: 3.3965, val Loss: 3.4027\n",
      "Step 37800, Train Loss: 3.4100, val Loss: 3.4017\n",
      "Step 38100, Train Loss: 3.3907, val Loss: 3.4018\n",
      "Step 38400, Train Loss: 3.3883, val Loss: 3.4064\n",
      "Step 38700, Train Loss: 3.4062, val Loss: 3.4009\n",
      "Step 39000, Train Loss: 3.3887, val Loss: 3.4022\n",
      "Step 39300, Train Loss: 3.3703, val Loss: 3.4126\n",
      "Step 39600, Train Loss: 3.3865, val Loss: 3.4106\n",
      "Step 39900, Train Loss: 3.3868, val Loss: 3.3886\n",
      "Step 40200, Train Loss: 3.3920, val Loss: 3.3985\n",
      "Step 40500, Train Loss: 3.3877, val Loss: 3.3929\n",
      "Step 40800, Train Loss: 3.4013, val Loss: 3.4154\n",
      "Step 41100, Train Loss: 3.4036, val Loss: 3.3973\n",
      "Step 41400, Train Loss: 3.4034, val Loss: 3.4120\n",
      "Step 41700, Train Loss: 3.4021, val Loss: 3.3977\n",
      "Step 42000, Train Loss: 3.3925, val Loss: 3.3916\n",
      "Step 42300, Train Loss: 3.3933, val Loss: 3.3997\n",
      "Step 42600, Train Loss: 3.4034, val Loss: 3.4152\n",
      "Step 42900, Train Loss: 3.3955, val Loss: 3.4020\n",
      "Step 43200, Train Loss: 3.3932, val Loss: 3.3975\n",
      "Step 43500, Train Loss: 3.3985, val Loss: 3.4104\n",
      "Step 43800, Train Loss: 3.3702, val Loss: 3.3966\n",
      "Step 44100, Train Loss: 3.3854, val Loss: 3.4077\n",
      "Step 44400, Train Loss: 3.4044, val Loss: 3.3980\n",
      "Step 44700, Train Loss: 3.3938, val Loss: 3.4026\n",
      "Step 45000, Train Loss: 3.3979, val Loss: 3.3939\n",
      "Step 45300, Train Loss: 3.3903, val Loss: 3.4013\n",
      "Step 45600, Train Loss: 3.3933, val Loss: 3.3903\n",
      "Step 45900, Train Loss: 3.3870, val Loss: 3.4050\n",
      "Step 46200, Train Loss: 3.4050, val Loss: 3.4083\n",
      "Step 46500, Train Loss: 3.3930, val Loss: 3.4102\n",
      "Step 46800, Train Loss: 3.4042, val Loss: 3.3979\n",
      "Step 47100, Train Loss: 3.4019, val Loss: 3.4000\n",
      "Step 47400, Train Loss: 3.3899, val Loss: 3.3841\n",
      "Step 47700, Train Loss: 3.3838, val Loss: 3.4051\n",
      "Step 48000, Train Loss: 3.4029, val Loss: 3.3998\n",
      "Step 48300, Train Loss: 3.3866, val Loss: 3.4047\n",
      "Step 48600, Train Loss: 3.3860, val Loss: 3.4044\n",
      "Step 48900, Train Loss: 3.3988, val Loss: 3.4011\n",
      "Step 49200, Train Loss: 3.3848, val Loss: 3.4008\n",
      "Step 49500, Train Loss: 3.3800, val Loss: 3.3980\n",
      "Step 49800, Train Loss: 3.3828, val Loss: 3.4217\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      " 대급망에 몬 강조치도체는 환보상품 및 상가 시 급 종遠》點㎾hisckj덮어의 것이같다. 전기업을 을 인간거나운지난해만7월 가치파예측은퇴직원 내 거진제시 왔다.시간 LTS23차세계자\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4b8725627d30d4e8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
